{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Pytorch実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polyphonic_data_loader as poly\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from util import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedTransition(nn.Module):\n",
    "    def __init__(self, z_dim, transition_dim):\n",
    "        super().__init__()\n",
    "        # こんなに層を重ねる理由は下のプログラムのコメント見ればわかる\n",
    "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)\n",
    "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)\n",
    "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
    "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
    "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, z_t_1):\n",
    "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))\n",
    "        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
    "        # proposed_meaやlocの計算式などは参照論文5節Deep Markov Modelの最後の方に\n",
    "        # 式があり， そこと同じ式になってる\n",
    "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))\n",
    "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
    "        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean\n",
    "        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, emission_dim):\n",
    "        super().__init__()\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
    "        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, z_t):\n",
    "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
    "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
    "        # ps:ベルヌーイ分布のパラメータ\n",
    "        ps = self.sigmoid(self.lin_hidden_to_input(h2))\n",
    "        return ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyroによる実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(self, mini_bacth, mini_batch_reversed, mini_batch_mask, \n",
    "        mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "    T_max = mini_batch.size(1)\n",
    "    pyro.module(\"dmm\", self)\n",
    "    # z_0\n",
    "    z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "    \n",
    "    '''\n",
    "    mini_batch: [バッチの次元, 一時的な次元（？）, 88次元(music data)]\n",
    "    '''\n",
    "    with pyro.plate(\"z_mini_batch\", len(mini_batch)):\n",
    "        for t in range(1, T_max+1):\n",
    "            z_loc, z_scale = self.trans(z_prev)\n",
    "            \n",
    "            with poutine.scale(None, annealing_factor):\n",
    "                z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                              dist.Normal(z_loc, z_scale)\n",
    "                                                          .mask(mini_batch_mask[:, t-1:t])\n",
    "                                                          .to_event(1))\n",
    "            \n",
    "            emission_probs_t = self.emitter(z_t)\n",
    "            pyro.sample(\"obs_x_%d\" % t,\n",
    "                                   dist.Bernoulli(emission_probs_t)\n",
    "                                          .mask(mini_batch_mask[:, t-1:t])\n",
    "                                          .to_event(1),\n",
    "                                          obs=mini_batch[:, t-1, :] )\n",
    "            z_prev = z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, z_t_1, h_rnn):\n",
    "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
    "        loc = self.lin_hidden_to_loc(h_combined)\n",
    "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask, \n",
    "                 mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "    T_max = mini_batch.size(1)\n",
    "    pyro.module(\"dmm\", self)\n",
    "    #  h_0\n",
    "    h_0_contig = self.h_0.expand(1, mini_batch.size(0),\n",
    "                                 self.rnn.hidden_size).contiguous()\n",
    "    rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "    rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "    z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "\n",
    "    with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "        for t in range(1, T_max + 1):\n",
    "            z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "            z_dist = dist.Normal(z_loc, z_scale)\n",
    "\n",
    "            with pyro.poutine.scale(None, annealing_factor):\n",
    "                z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                  z_dist.mask(mini_batch_mask[:, t - 1:t])\n",
    "                                        .to_event(1))\n",
    "            z_prev = z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMM(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module encapsulates the model as well as the\n",
    "    variational distribution (the guide) for the Deep Markov Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=88, z_dim=100, emission_dim=100,\n",
    "                 transition_dim=200, rnn_dim=600, num_layers=1, rnn_dropout_rate=0.0,\n",
    "                 num_iafs=0, iaf_dim=50, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.emitter = Emitter(input_dim, z_dim, emission_dim)\n",
    "        self.trans = GatedTransition(z_dim, transition_dim)\n",
    "        self.combiner = Combiner(z_dim, rnn_dim)\n",
    "        rnn_dropout_rate = 0. if num_layers == 1 else rnn_dropout_rate\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, bidirectional=False, num_layers=num_layers,\n",
    "                          dropout=rnn_dropout_rate)\n",
    "\n",
    "        self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]\n",
    "        self.iafs_modules = nn.ModuleList(self.iafs)\n",
    "        self.z_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
    "        self.use_cuda = use_cuda\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "\n",
    "    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "        T_max = mini_batch.size(1)\n",
    "        pyro.module(\"dmm\", self)\n",
    "        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))\n",
    "        \n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                z_loc, z_scale = self.trans(z_prev)\n",
    "                \n",
    "                with poutine.scale(scale=annealing_factor):\n",
    "                    z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                      dist.Normal(z_loc, z_scale)\n",
    "                                          .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                          .to_event(1))\n",
    "                emission_probs_t = self.emitter(z_t)\n",
    "                pyro.sample(\"obs_x_%d\" % t,\n",
    "                            dist.Bernoulli(emission_probs_t)\n",
    "                                .mask(mini_batch_mask[:, t - 1:t])\n",
    "                                .to_event(1),\n",
    "                            obs=mini_batch[:, t - 1, :])\n",
    "                z_prev = z_t\n",
    "\n",
    "    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "              mini_batch_seq_lengths, annealing_factor=1.0):\n",
    "        T_max = mini_batch.size(1)\n",
    "        pyro.module(\"dmm\", self)\n",
    "\n",
    "        h_0_contig = self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()\n",
    "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
    "        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)\n",
    "        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))\n",
    "\n",
    "        with pyro.plate(\"z_minibatch\", len(mini_batch)):\n",
    "            for t in pyro.markov(range(1, T_max + 1)):\n",
    "                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
    "                if len(self.iafs) > 0:\n",
    "                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)\n",
    "                    assert z_dist.event_shape == (self.z_q_0.size(0),)\n",
    "                    assert z_dist.batch_shape[-1:] == (len(mini_batch),)\n",
    "                else:\n",
    "                    z_dist = dist.Normal(z_loc, z_scale)\n",
    "                    assert z_dist.event_shape == ()\n",
    "                    assert z_dist.batch_shape[-2:] == (len(mini_batch), self.z_q_0.size(0))\n",
    "\n",
    "                with pyro.poutine.scale(scale=annealing_factor):\n",
    "                    if len(self.iafs) > 0:\n",
    "                        z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                          z_dist.mask(mini_batch_mask[:, t - 1]))\n",
    "                    else:\n",
    "                        z_t = pyro.sample(\"z_%d\" % t,\n",
    "                                          z_dist.mask(mini_batch_mask[:, t - 1:t])\n",
    "                                          .to_event(1))\n",
    "                z_prev = z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmm = DMM()\n",
    "\n",
    "adam_params = {\"lr\": 0.0003, \"betas\": (0.96, 0.999),\n",
    "               \"clip_norm\": 10.0, \"lrd\": 0.99996,\n",
    "               \"weight_decay\": 2.0}\n",
    "optimizer = ClippedAdam(adam_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svi = SVI(dmm.model, dmm.guide, optimizer, Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_minibatch(epoch, which_mini_batch, shuffled_indices, annealing_epochs=1000, minimum_annealing_factor=0.2):\n",
    "        if annealing_epochs > 0 and epoch < annealing_epochs:\n",
    "            # compute the KL annealing factor approriate for the current mini-batch in the current epoch\n",
    "            min_af = minimum_annealing_factor\n",
    "            annealing_factor = min_af + (1.0 - min_af) * \\\n",
    "                (float(which_mini_batch + epoch * N_mini_batches + 1) /\n",
    "                 float(annealing_epochs * N_mini_batches))\n",
    "        else:\n",
    "            # by default the KL annealing factor is unity\n",
    "            annealing_factor = 1.0\n",
    "\n",
    "        # compute which sequences in the training set we should grab\n",
    "        mini_batch_start = (which_mini_batch * mini_batch_size)\n",
    "        mini_batch_end = np.min([(which_mini_batch + 1) * mini_batch_size, N_train_data])\n",
    "        mini_batch_indices = shuffled_indices[mini_batch_start:mini_batch_end]\n",
    "        # grab a fully prepped mini-batch using the helper function in the data loader\n",
    "        mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths \\\n",
    "            = poly.get_mini_batch(mini_batch_indices, training_data_sequences,\n",
    "                                  training_seq_lengths, cuda=False)\n",
    "        # do an actual gradient step\n",
    "        loss = svi.step(mini_batch, mini_batch_reversed, mini_batch_mask,\n",
    "                        mini_batch_seq_lengths, annealing_factor)\n",
    "        # keep track of the training loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_evaluation():\n",
    "        # put the RNN into evaluation mode (i.e. turn off drop-out if applicable)\n",
    "        dmm.rnn.eval()\n",
    "\n",
    "        # compute the validation and test loss n_samples many times\n",
    "        val_nll = svi.evaluate_loss(val_batch, val_batch_reversed, val_batch_mask,\n",
    "                                    val_seq_lengths) / float(torch.sum(val_seq_lengths))\n",
    "        test_nll = svi.evaluate_loss(test_batch, test_batch_reversed, test_batch_mask,\n",
    "                                     test_seq_lengths) / float(torch.sum(test_seq_lengths))\n",
    "\n",
    "        # put the RNN back into training mode (i.e. turn on drop-out if applicable)\n",
    "        dmm.rnn.train()\n",
    "        return val_nll, test_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N_train_data: 229     avg. training seq. length: 60.29    N_mini_batches: 12\n",
      "N_train_data: 229     avg. training seq. length: 60.29    N_mini_batches: 12\n",
      "N_train_data: 229     avg. training seq. length: 60.29    N_mini_batches: 12\n"
     ]
    }
   ],
   "source": [
    "log = get_logger('dmm.log')\n",
    "mini_batch_size = 20\n",
    "data = poly.load_data(poly.JSB_CHORALES)\n",
    "training_seq_lengths = data['train']['sequence_lengths']\n",
    "training_data_sequences = data['train']['sequences']\n",
    "test_seq_lengths = data['test']['sequence_lengths']\n",
    "test_data_sequences = data['test']['sequences']\n",
    "val_seq_lengths = data['valid']['sequence_lengths']\n",
    "val_data_sequences = data['valid']['sequences']\n",
    "N_train_data = len(training_seq_lengths)\n",
    "N_train_time_slices = float(torch.sum(training_seq_lengths))\n",
    "N_mini_batches = int(N_train_data / mini_batch_size +\n",
    "                     int(N_train_data % mini_batch_size > 0))\n",
    "\n",
    "log(\"N_train_data: %d     avg. training seq. length: %.2f    N_mini_batches: %d\" %\n",
    "    (N_train_data, training_seq_lengths.float().mean(), N_mini_batches))\n",
    "\n",
    "# how often we do validation/test evaluation during training\n",
    "val_test_frequency = 50\n",
    "# the number of samples we use to do the evaluation\n",
    "n_eval_samples = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[training epoch 0000]  58.8030 \t\t\t\t(dt = 12.886 sec)\n",
      "[training epoch 0000]  58.8030 \t\t\t\t(dt = 12.886 sec)\n",
      "[training epoch 0000]  58.8030 \t\t\t\t(dt = 12.886 sec)\n",
      "[training epoch 0001]  46.2033 \t\t\t\t(dt = 12.532 sec)\n",
      "[training epoch 0001]  46.2033 \t\t\t\t(dt = 12.532 sec)\n",
      "[training epoch 0001]  46.2033 \t\t\t\t(dt = 12.532 sec)\n",
      "[training epoch 0002]  25.2181 \t\t\t\t(dt = 12.284 sec)\n",
      "[training epoch 0002]  25.2181 \t\t\t\t(dt = 12.284 sec)\n",
      "[training epoch 0002]  25.2181 \t\t\t\t(dt = 12.284 sec)\n",
      "[training epoch 0003]  17.2768 \t\t\t\t(dt = 12.438 sec)\n",
      "[training epoch 0003]  17.2768 \t\t\t\t(dt = 12.438 sec)\n",
      "[training epoch 0003]  17.2768 \t\t\t\t(dt = 12.438 sec)\n",
      "[training epoch 0004]  15.1741 \t\t\t\t(dt = 12.584 sec)\n",
      "[training epoch 0004]  15.1741 \t\t\t\t(dt = 12.584 sec)\n",
      "[training epoch 0004]  15.1741 \t\t\t\t(dt = 12.584 sec)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "# train_loop\n",
    "times = [time.time()]\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # accumulator for our estimate of the negative log likelihood (or rather -elbo) for this epoch\n",
    "    epoch_nll = 0.0\n",
    "    # prepare mini-batch subsampling indices for this epoch\n",
    "    shuffled_indices = torch.randperm(N_train_data)\n",
    "\n",
    "    # process each mini-batch; this is where we take gradient steps\n",
    "    for which_mini_batch in range(N_mini_batches):\n",
    "        epoch_nll += process_minibatch(epoch, which_mini_batch, shuffled_indices)\n",
    "\n",
    "    # report training diagnostics\n",
    "    times.append(time.time())\n",
    "    epoch_time = times[-1] - times[-2]\n",
    "    log(\"[training epoch %04d]  %.4f \\t\\t\\t\\t(dt = %.3f sec)\" %\n",
    "        (epoch, epoch_nll / N_train_time_slices, epoch_time))\n",
    "\n",
    "    # do evaluation on test and validation data and report results\n",
    "    if val_test_frequency > 0 and epoch > 0 and epoch % val_test_frequency == 0:\n",
    "        val_nll, test_nll = do_evaluation()\n",
    "        log(\"[val/test epoch %04d]  %.4f  %.4f\" % (epoch, val_nll, test_nll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
