{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import dmm.polyphonic_data_loader as poly\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO, TraceTMC_ELBO\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.optim import Adam\n",
    "from pyro.util import ignore_jit_warnings\n",
    "\n",
    "logging.basicConfig(format='%(relativeCreated) 9d %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger()\n",
    "debug_handler = logging.StreamHandler(sys.stdout)\n",
    "debug_handler.setLevel(logging.DEBUG)\n",
    "debug_handler.addFilter(filter=lambda record: record.levelno <= logging.DEBUG)\n",
    "log.addHandler(debug_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_0(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    assert not torch._C._get_tracing_state()\n",
    "    num_sequences, max_length, data_dim = sequences.shape\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        # Our prior on transition probabilities will be:\n",
    "        # stay in the same state with 90% probability; uniformly jump to another\n",
    "        # state with 10% probability.\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        # We put a weak prior on the conditional probability of a tone sounding.\n",
    "        # We know that on average about 4 of 88 tones are active, so we'll set a\n",
    "        # rough weak prior of 10% of the notes being active at any one time.\n",
    "        probs_y = pyro.sample(\"probs_y\",\n",
    "                              dist.Beta(0.1, 0.9)\n",
    "                                  .expand([args.hidden_dim, data_dim])\n",
    "                                  .to_event(2))\n",
    "    # In this first model we'll sequentially iterate over sequences in a\n",
    "    # minibatch; this will make it easy to reason about tensor shapes.\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    for i in pyro.plate(\"sequences\", len(sequences), batch_size):\n",
    "        length = lengths[i]\n",
    "        sequence = sequences[i, :length]\n",
    "        x = 0\n",
    "        for t in pyro.markov(range(length)):\n",
    "            # On the next line, we'll overwrite the value of x with an updated\n",
    "            # value. If we wanted to record all x values, we could instead\n",
    "            # write x[t] = pyro.sample(...x[t-1]...).\n",
    "            x = pyro.sample(\"x_{}_{}\".format(i, t), dist.Categorical(probs_x[x]),\n",
    "                            infer={\"enumerate\": \"parallel\"})\n",
    "            with tones_plate:\n",
    "                pyro.sample(\"y_{}_{}\".format(i, t), dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                            obs=sequence[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    # Sometimes it is safe to ignore jit warnings. Here we use the\n",
    "    # pyro.util.ignore_jit_warnings context manager to silence warnings about\n",
    "    # conversion to integer, since we know all three numbers will be the same\n",
    "    # across all invocations to the model.\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        probs_y = pyro.sample(\"probs_y\",\n",
    "                              dist.Beta(0.1, 0.9)\n",
    "                                  .expand([args.hidden_dim, data_dim])\n",
    "                                  .to_event(2))\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    # We subsample batch_size items out of num_sequences items. Note that since\n",
    "    # we're using dim=-1 for the notes plate, we need to batch over a different\n",
    "    # dimension, here dim=-2.\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x = 0\n",
    "        # If we are not using the jit, then we can vary the program structure\n",
    "        # each call by running for a dynamically determined number of time\n",
    "        # steps, lengths.max(). However if we are using the jit, then we try to\n",
    "        # keep a single program structure for all minibatches; the fixed\n",
    "        # structure ends up being faster since each program structure would\n",
    "        # need to trigger a new jit compile stage.\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\"x_{}\".format(t), dist.Categorical(probs_x[x]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                with tones_plate:\n",
    "                    pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y[x.squeeze(-1)]),\n",
    "                                obs=sequences[batch, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        probs_y = pyro.sample(\"probs_y\",\n",
    "                              dist.Beta(0.1, 0.9)\n",
    "                                  .expand([args.hidden_dim, 2, data_dim])\n",
    "                                  .to_event(3))\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x, y = 0, 0\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\"x_{}\".format(t), dist.Categorical(probs_x[x]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                # Note the broadcasting tricks here: to index probs_y on tensors x and y,\n",
    "                # we also need a final tensor for the tones dimension. This is conveniently\n",
    "                # provided by the plate associated with that dimension.\n",
    "                with tones_plate as tones:\n",
    "                    y = pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y[x, y, tones]),\n",
    "                                    obs=sequences[batch, t]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_3(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    hidden_dim = int(args.hidden_dim ** 0.5)  # split between w and x\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_w = pyro.sample(\"probs_w\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        probs_y = pyro.sample(\"probs_y\",\n",
    "                              dist.Beta(0.1, 0.9)\n",
    "                                  .expand([hidden_dim, hidden_dim, data_dim])\n",
    "                                  .to_event(3))\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        w, x = 0, 0\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                w = pyro.sample(\"w_{}\".format(t), dist.Categorical(probs_w[w]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                x = pyro.sample(\"x_{}\".format(t), dist.Categorical(probs_x[x]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                with tones_plate as tones:\n",
    "                    pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y[w, x, tones]),\n",
    "                                obs=sequences[batch, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_4(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "    hidden_dim = int(args.hidden_dim ** 0.5)  # split between w and x\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_w = pyro.sample(\"probs_w\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(hidden_dim) + 0.1)\n",
    "                                  .expand_by([hidden_dim])\n",
    "                                  .to_event(2))\n",
    "        probs_y = pyro.sample(\"probs_y\",\n",
    "                              dist.Beta(0.1, 0.9)\n",
    "                                  .expand([hidden_dim, hidden_dim, data_dim])\n",
    "                                  .to_event(3))\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        # Note the broadcasting tricks here: we declare a hidden torch.arange and\n",
    "        # ensure that w and x are always tensors so we can unsqueeze them below,\n",
    "        # thus ensuring that the x sample sites have correct distribution shape.\n",
    "        w = x = torch.tensor(0, dtype=torch.long)\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                w = pyro.sample(\"w_{}\".format(t), dist.Categorical(probs_w[w]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                x = pyro.sample(\"x_{}\".format(t),\n",
    "                                dist.Categorical(Vindex(probs_x)[w, x]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                with tones_plate as tones:\n",
    "                    pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y[w, x, tones]),\n",
    "                                obs=sequences[batch, t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TonesGenerator(nn.Module):\n",
    "    def __init__(self, args, data_dim):\n",
    "        self.args = args\n",
    "        self.data_dim = data_dim\n",
    "        super().__init__()\n",
    "        self.x_to_hidden = nn.Linear(args.hidden_dim, args.nn_dim)\n",
    "        self.y_to_hidden = nn.Linear(args.nn_channels * data_dim, args.nn_dim)\n",
    "        self.conv = nn.Conv1d(1, args.nn_channels, 3, padding=1)\n",
    "        self.hidden_to_logits = nn.Linear(args.nn_dim, data_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Hidden units depend on two inputs: a one-hot encoded categorical variable x, and\n",
    "        # a bernoulli variable y. Whereas x will typically be enumerated, y will be observed.\n",
    "        # We apply x_to_hidden independently from y_to_hidden, then broadcast the non-enumerated\n",
    "        # y part up to the enumerated x part in the + operation.\n",
    "        x_onehot = y.new_zeros(x.shape[:-1] + (self.args.hidden_dim,)).scatter_(-1, x, 1)\n",
    "        y_conv = self.relu(self.conv(y.reshape(-1, 1, self.data_dim))).reshape(y.shape[:-1] + (-1,))\n",
    "        h = self.relu(self.x_to_hidden(x_onehot) + self.y_to_hidden(y_conv))\n",
    "        return self.hidden_to_logits(h)\n",
    "\n",
    "\n",
    "# We will create a single global instance later.\n",
    "tones_generator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_5(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    # Initialize a global module instance if needed.\n",
    "    global tones_generator\n",
    "    if tones_generator is None:\n",
    "        tones_generator = TonesGenerator(args, data_dim)\n",
    "    pyro.module(\"tones_generator\", tones_generator)\n",
    "\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x = 0\n",
    "        y = torch.zeros(data_dim)\n",
    "        for t in pyro.markov(range(max_length if args.jit else lengths.max())):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                x = pyro.sample(\"x_{}\".format(t), dist.Categorical(probs_x[x]),\n",
    "                                infer={\"enumerate\": \"parallel\"})\n",
    "                # Note that since each tone depends on all tones at a previous time step\n",
    "                # the tones at different time steps now need to live in separate plates.\n",
    "                with pyro.plate(\"tones_{}\".format(t), data_dim, dim=-1):\n",
    "                    y = pyro.sample(\"y_{}\".format(t),\n",
    "                                    dist.Bernoulli(logits=tones_generator(x, y)),\n",
    "                                    obs=sequences[batch, t])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_6(sequences, lengths, args, batch_size=None, include_prior=False):\n",
    "    num_sequences, max_length, data_dim = sequences.shape\n",
    "    assert lengths.shape == (num_sequences,)\n",
    "    assert lengths.max() <= max_length\n",
    "    hidden_dim = args.hidden_dim\n",
    "\n",
    "    if not args.raftery_parameterization:\n",
    "        # Explicitly parameterize the full tensor of transition probabilities, which\n",
    "        # has hidden_dim cubed entries.\n",
    "        probs_x = pyro.param(\"probs_x\", torch.rand(hidden_dim, hidden_dim, hidden_dim),\n",
    "                             constraint=constraints.simplex)\n",
    "    else:\n",
    "        # Use the more parsimonious \"Raftery\" parameterization of\n",
    "        # the tensor of transition probabilities. See reference:\n",
    "        # Raftery, A. E. A model for high-order markov chains.\n",
    "        # Journal of the Royal Statistical Society. 1985.\n",
    "        probs_x1 = pyro.param(\"probs_x1\", torch.rand(hidden_dim, hidden_dim),\n",
    "                              constraint=constraints.simplex)\n",
    "        probs_x2 = pyro.param(\"probs_x2\", torch.rand(hidden_dim, hidden_dim),\n",
    "                              constraint=constraints.simplex)\n",
    "        mix_lambda = pyro.param(\"mix_lambda\", torch.tensor(0.5), constraint=constraints.unit_interval)\n",
    "        # we use broadcasting to combine two tensors of shape (hidden_dim, hidden_dim) and\n",
    "        # (hidden_dim, 1, hidden_dim) to obtain a tensor of shape (hidden_dim, hidden_dim, hidden_dim)\n",
    "        probs_x = mix_lambda * probs_x1 + (1.0 - mix_lambda) * probs_x2.unsqueeze(-2)\n",
    "\n",
    "    probs_y = pyro.param(\"probs_y\", torch.rand(hidden_dim, data_dim),\n",
    "                         constraint=constraints.unit_interval)\n",
    "    tones_plate = pyro.plate(\"tones\", data_dim, dim=-1)\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-2) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        x_curr, x_prev = torch.tensor(0), torch.tensor(0)\n",
    "        # we need to pass the argument `history=2' to `pyro.markov()`\n",
    "        # since our model is now 2-markov\n",
    "        for t in pyro.markov(range(lengths.max()), history=2):\n",
    "            with poutine.mask(mask=(t < lengths).unsqueeze(-1)):\n",
    "                probs_x_t = Vindex(probs_x)[x_prev, x_curr]\n",
    "                x_prev, x_curr = x_curr, pyro.sample(\"x_{}\".format(t), dist.Categorical(probs_x_t),\n",
    "                                                     infer={\"enumerate\": \"parallel\"})\n",
    "                with tones_plate:\n",
    "                    probs_y_t = probs_y[x_curr.squeeze(-1)]\n",
    "                    pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y_t),\n",
    "                                obs=sequences[batch, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_7(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences,)\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    # Initialize a global module instance if needed.\n",
    "    global tones_generator\n",
    "    if tones_generator is None:\n",
    "        tones_generator = TonesGenerator(args, data_dim)\n",
    "    pyro.module(\"tones_generator\", tones_generator)\n",
    "\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        probs_x = pyro.sample(\"probs_x\",\n",
    "                              dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1)\n",
    "                                  .to_event(1))\n",
    "    with pyro.plate(\"sequences\", num_sequences, batch_size, dim=-1) as batch:\n",
    "        lengths = lengths[batch]\n",
    "        y = sequences[batch] if args.jit else sequences[batch, :lengths.max()]\n",
    "        x = torch.arange(args.hidden_dim)\n",
    "        t = torch.arange(y.size(1))\n",
    "        init_logits = torch.full((args.hidden_dim,), -float('inf'))\n",
    "        init_logits[0] = 0\n",
    "        trans_logits = probs_x.log()\n",
    "        with ignore_jit_warnings():\n",
    "            obs_dist = dist.Bernoulli(logits=tones_generator(x, y.unsqueeze(-2))).to_event(1)\n",
    "            obs_dist = obs_dist.mask((t < lengths.unsqueeze(-1)).unsqueeze(-1))\n",
    "            hmm_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n",
    "        pyro.sample(\"y\", hmm_dist, obs=y)\n",
    "\n",
    "\n",
    "models = {name[len('model_'):]: model\n",
    "          for name, model in globals().items()\n",
    "          if name.startswith('model_')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if args.cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "    logging.info('Loading data')\n",
    "    data = poly.load_data(poly.JSB_CHORALES)\n",
    "\n",
    "    logging.info('-' * 40)\n",
    "    model = models[args.model]\n",
    "    logging.info('Training {} on {} sequences'.format(\n",
    "        model.__name__, len(data['train']['sequences'])))\n",
    "    sequences = data['train']['sequences']\n",
    "    lengths = data['train']['sequence_lengths']\n",
    "\n",
    "    # find all the notes that are present at least once in the training set\n",
    "    present_notes = ((sequences == 1).sum(0).sum(0) > 0)\n",
    "    # remove notes that are never played (we remove 37/88 notes)\n",
    "    sequences = sequences[..., present_notes]\n",
    "\n",
    "    if args.truncate:\n",
    "        lengths = lengths.clamp(max=args.truncate)\n",
    "        sequences = sequences[:, :args.truncate]\n",
    "    num_observations = float(lengths.sum())\n",
    "    pyro.set_rng_seed(args.seed)\n",
    "    pyro.clear_param_store()\n",
    "    pyro.enable_validation(__debug__)\n",
    "\n",
    "    # We'll train using MAP Baum-Welch, i.e. MAP estimation while marginalizing\n",
    "    # out the hidden state x. This is accomplished via an automatic guide that\n",
    "    # learns point estimates of all of our conditional probability tables,\n",
    "    # named probs_*.\n",
    "    guide = AutoDelta(poutine.block(model, expose_fn=lambda msg: msg[\"name\"].startswith(\"probs_\")))\n",
    "\n",
    "    # To help debug our tensor shapes, let's print the shape of each site's\n",
    "    # distribution, value, and log_prob tensor. Note this information is\n",
    "    # automatically printed on most errors inside SVI.\n",
    "    if args.print_shapes:\n",
    "        first_available_dim = -2 if model is model_0 else -3\n",
    "        guide_trace = poutine.trace(guide).get_trace(\n",
    "            sequences, lengths, args=args, batch_size=args.batch_size)\n",
    "        model_trace = poutine.trace(\n",
    "            poutine.replay(poutine.enum(model, first_available_dim), guide_trace)).get_trace(\n",
    "            sequences, lengths, args=args, batch_size=args.batch_size)\n",
    "        logging.info(model_trace.format_shapes())\n",
    "\n",
    "    # Enumeration requires a TraceEnum elbo and declaring the max_plate_nesting.\n",
    "    # All of our models have two plates: \"data\" and \"tones\".\n",
    "    optim = Adam({'lr': args.learning_rate})\n",
    "    if args.tmc:\n",
    "        if args.jit:\n",
    "            raise NotImplementedError(\"jit support not yet added for TraceTMC_ELBO\")\n",
    "        elbo = TraceTMC_ELBO(max_plate_nesting=1 if model is model_0 else 2)\n",
    "        tmc_model = poutine.infer_config(\n",
    "            model,\n",
    "            lambda msg: {\"num_samples\": args.tmc_num_samples, \"expand\": False} if msg[\"infer\"].get(\"enumerate\", None) == \"parallel\" else {})  # noqa: E501\n",
    "        svi = SVI(tmc_model, guide, optim, elbo)\n",
    "    else:\n",
    "        Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n",
    "        elbo = Elbo(max_plate_nesting=1 if model is model_0 else 2,\n",
    "                    strict_enumeration_warning=(model is not model_7),\n",
    "                    jit_options={\"time_compilation\": args.time_compilation})\n",
    "        svi = SVI(model, guide, optim, elbo)\n",
    "\n",
    "    # We'll train on small minibatches.\n",
    "    logging.info('Step\\tLoss')\n",
    "    for step in range(args.num_steps):\n",
    "        loss = svi.step(sequences, lengths, args=args, batch_size=args.batch_size)\n",
    "        logging.info('{: >5d}\\t{}'.format(step, loss / num_observations))\n",
    "\n",
    "    if args.jit and args.time_compilation:\n",
    "        logging.debug('time to compile: {} s.'.format(elbo._differentiable_loss.compile_time))\n",
    "\n",
    "    # We evaluate on the entire training dataset,\n",
    "    # excluding the prior term so our results are comparable across models.\n",
    "    train_loss = elbo.loss(model, guide, sequences, lengths, args, include_prior=False)\n",
    "    logging.info('training loss = {}'.format(train_loss / num_observations))\n",
    "\n",
    "    # Finally we evaluate on the test dataset.\n",
    "    logging.info('-' * 40)\n",
    "    logging.info('Evaluating on {} test sequences'.format(len(data['test']['sequences'])))\n",
    "    sequences = data['test']['sequences'][..., present_notes]\n",
    "    lengths = data['test']['sequence_lengths']\n",
    "    if args.truncate:\n",
    "        lengths = lengths.clamp(max=args.truncate)\n",
    "    num_observations = float(lengths.sum())\n",
    "\n",
    "    # note that since we removed unseen notes above (to make the problem a bit easier and for\n",
    "    # numerical stability) this test loss may not be directly comparable to numbers\n",
    "    # reported on this dataset elsewhere.\n",
    "    test_loss = elbo.loss(model, guide, sequences, lengths, args=args, include_prior=False)\n",
    "    logging.info('test loss = {}'.format(test_loss / num_observations))\n",
    "\n",
    "    # We expect models with higher capacity to perform better,\n",
    "    # but eventually overfit to the training set.\n",
    "    capacity = sum(value.reshape(-1).size(0)\n",
    "                   for value in pyro.get_param_store().values())\n",
    "    logging.info('{} capacity = {} parameters'.format(model.__name__, capacity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-m MODEL] [-n NUM_STEPS] [-b BATCH_SIZE]\n",
      "                             [-d HIDDEN_DIM] [-nn NN_DIM] [-nc NN_CHANNELS]\n",
      "                             [-lr LEARNING_RATE] [-t TRUNCATE] [-p]\n",
      "                             [--seed SEED] [--cuda] [--jit]\n",
      "                             [--time-compilation] [-rp] [--tmc]\n",
      "                             [--tmc-num-samples TMC_NUM_SAMPLES]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/ikumaoka/Library/Jupyter/runtime/kernel-9210b8ee-86dd-43fc-9f2b-53ac785ce7bd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ikumaoka/.pyenv/versions/anaconda3-2019.07/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3333: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"MAP Baum-Welch learning Bach Chorales\")\n",
    "    parser.add_argument(\"-m\", \"--model\", default=\"1\", type=str,\n",
    "                        help=\"one of: {}\".format(\", \".join(sorted(models.keys()))))\n",
    "    parser.add_argument(\"-n\", \"--num-steps\", default=50, type=int)\n",
    "    parser.add_argument(\"-b\", \"--batch-size\", default=8, type=int)\n",
    "    parser.add_argument(\"-d\", \"--hidden-dim\", default=16, type=int)\n",
    "    parser.add_argument(\"-nn\", \"--nn-dim\", default=48, type=int)\n",
    "    parser.add_argument(\"-nc\", \"--nn-channels\", default=2, type=int)\n",
    "    parser.add_argument(\"-lr\", \"--learning-rate\", default=0.05, type=float)\n",
    "    parser.add_argument(\"-t\", \"--truncate\", type=int)\n",
    "    parser.add_argument(\"-p\", \"--print-shapes\", action=\"store_true\")\n",
    "    parser.add_argument(\"--seed\", default=0, type=int)\n",
    "    parser.add_argument('--cuda', action='store_true')\n",
    "    parser.add_argument('--jit', action='store_true')\n",
    "    parser.add_argument('--time-compilation', action='store_true')\n",
    "    parser.add_argument('-rp', '--raftery-parameterization', action='store_true')\n",
    "    parser.add_argument('--tmc', action='store_true',\n",
    "                        help=\"Use Tensor Monte Carlo instead of exact enumeration \"\n",
    "                             \"to estimate the marginal likelihood. You probably don't want to do this, \"\n",
    "                             \"except to see that TMC makes Monte Carlo gradient estimation feasible \"\n",
    "                             \"even with very large numbers of non-reparametrized variables.\")\n",
    "    parser.add_argument('--tmc-num-samples', default=10, type=int)\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
